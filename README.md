## Project Milestone 4 - E115 - Birdwatching App
   
#### Project Milestone 4 Organization

```
â”œâ”€â”€ Readme.md
â”œâ”€â”€ images 
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ BirdWatchingApp.ipynb
â”‚   â”œâ”€â”€ Acoustic_Monitoring_EDA.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Biodiversity.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Bird_Locations.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Deforestation.ipynb
â”‚   â”œâ”€â”€ SemanticScholar.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â””â”€â”€ preprocess_cv.py
â”‚   
â”œâ”€â”€ references
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ BirdWatchingAppMidterm.pdf
â”‚   â””â”€â”€ Statement of Work_Sample.pdf 
â”‚
â””â”€â”€ src  
    â”œâ”€â”€ api-service
    â”‚    â”œâ”€â”€ api
    â”‚    â”‚   â”œâ”€â”€ routers
    â”‚    â”‚   â”œâ”€â”€ utils
    â”‚    â”‚   â””â”€â”€ service.py
    â”‚    â”œâ”€â”€ docker-entrypoint.sh
    â”‚    â”œâ”€â”€ docker-shell.sh
    â”‚    â”œâ”€â”€ Dockerfile
    â”‚    â”œâ”€â”€ Pipfile
    â”‚    â”œâ”€â”€ Pipfile.lock
    â”œâ”€â”€ frontend-react
    â”‚    â”œâ”€â”€ public
    â”‚    â”œâ”€â”€ src
    â”‚    â”œâ”€â”€ app
    â”‚    â”œ   â”œâ”€â”€ chat 
    â”‚    â”œ   â”œâ”€â”€ sounds explorer
    â”‚    â”œ   â””â”€â”€ interactive maps
    â”‚    â”œâ”€â”€ components 
    â”‚    â”œâ”€â”€ service 
    â”‚    â”œâ”€â”€ Dockerfile
    â”‚    â””â”€â”€ docker-shell.sh
    â””â”€â”€ vector-db
         â”œâ”€â”€ agent_tools.py
         â”œâ”€â”€ cli.py
         â”œâ”€â”€ docker-compose.yml
         â”œâ”€â”€ docker-entrypoint.sh
         â”œâ”€â”€ Dockerfile
         â””â”€â”€ docker-shell.sh
    

```

# E115 - Milestone4 - Birdwatching App

**Team Members:** Jaqueline Garcia-Yi, Susan Urban, Yong Li, and Victoria Okereke

**Group Name:** Birdwatching App

**Project:**  
This project leverages AI to support bird species identification, using Yanachaga ChemillÃ©n National Park in Peru as a case study. The park is home to over 500 bird species, many of which are endemic (found nowhere else in the world). The app is powered by an AI-based acoustic model trained to identify bird species through their vocalizations. It also features interactive maps enriched with environmental and habitat data, such as deforestation and biodiversity indicators. Additionally, a large language model (LLM) agent serves as a virtual bird expert, offering detailed information about identified species and answering bird-related queries. This project highlights the integration of AI prediction models, natural language processing, and geospatial data for applications in ecotourism, environmental monitoring, and education.  
<br><br>

----

## Milestone4 ##

In this milestone, we have the components for frontend, API service, also components from previous milestones for data management, including versioning, as well as the interactive maps, and acoustic and language models.

After completions of building a robust ML Pipeline in our previous milestone we have built a backend api service and frontend app. This will be our user-facing application that ties together the various components built in previous milestones.

### 1. Application Design ###

Before we start implementing the app we built a detailed design document outlining the applicationâ€™s architecture. We built a Solution Architecture and Technical Architecture to ensure all our components work together.

Here is our Solution Architecture:

<img src="images/solution-arch.png"  width="800">

Here is our Technical Architecture:

<img src="images/technical-arch.png"  width="800">

The architectures follow a state of the art design using enterprise COTS and open source products. A Google Earth API is added in support of the several maps used in the app. 

<<<<<<< HEAD
<<<<<<< HEAD

**Backend API**
=======
=======

>>>>>>> bd1f5cc3af336ff388b60770c88bb8e530c3ad53
### 2. Backend API ###
>>>>>>> a60db3e4c99205c015520f1ad3e40bd4c2b3ac47

The backend API is built using FastAPI and serves as the core interface between the frontend, the BirdNET model, and the LLM agent. It processes both audio and text inputs and routes them through intelligent workflows designed to enhance the birdwatching experience. It allows audio-based species detection by accepting bird audio recordings and processes them through the BirdNET model to identify the bird species. It also supports natural language inputs from users and sends them to the LLM agent for answers. It wraps BirdNET predictions with informative responses generated by the LLM agent, returning a rich description to the user. The backend API further provides endpoints for checking model status and overall API health.

<img src="images/api-list.png"  width="800">

**2.1. Acoustic Model for Bird Species Identification**

The frontend will allow an audio file uploaded with a limitation of 5MB, and the backend api llm_cnn_chat.py will save the file in a temperory path and pass the temp file path to BirdNET model. Without any preprocessing, the BirdNET model use its built-in preprocessor to chunk the input audio into fixed length (3 or 5 seconds) pieces, and convert each of small piece into a spectrogram by Short Time Fourier Transformation. The spectrogram is represented in both time and frequency domain, can be treated as "image" data (see 5.2 Notebook). Each image data chunk pass to the BirdNET neural network model and generate an embedding. So for an audio recording longer than fixed chunk length, the BirdNET model will generate more than one embeddings, and give prediction for each embedding or each chunk of data. The prediction confidence on each species are averaged over all chunks, and the results are ranked and get the prediction with highest confidence. The api llm_cnn_chat could answer question about the species related to habitat preferences, feeding behaviors and dietary needs, breeding cycles and nesting habits, conservation strategies and threats etc. The questions can be either when the audio is uploaded or after the model predict the audio input. 

**2.2. Bird Knowledge Expert (LLM-Agent Chatbot)**

The LLM agent is the conversational layer that brings context and insight to the app. Whether responding to text-based questions or enhancing audio-based bird species predictions, it serves up detailed, accurate, and engaging information.
Key features:
-   RAG-Enhanced Response Generation: The agent uses a Retrieval-Augmented Generation (RAG) architecture. It includes a custom tool that retrieves relevant bird species data from our web-scrapped, chunked, and embedded knowledge base, then passes that context into the LLM to generate informed, natural responses.
-   Audio Input Pipeline: When an audio file is received, the BirdNET model predicts the most likely species. This prediction is used to generate a search query, retrieve species info, and craft a human-friendly explanation using the LLM.
-   Text Input Support: For direct user questions (e.g., "What does a cardinal sound like?" or "Where can I spot a robin?"), the agent dynamically pulls and injects relevant information to respond meaningfully.
-   Tool-Augmented Architecture
 The agent supports tools like ```get_specie_info_by_search_content```, which filters and ranks knowledge base content using cosine similarity against expanded user queries.

### 3. Frontend React ###
The frontend of the application is built as a user-friendly React web interface, designed to enable users to identify bird species through audio recordings and access detailed bird information via an AI agent. The primary functionality centers around integrating with a backend service that hosts a BirdNET-based acoustic model for species prediction and a large language model (LLM) acting as a bird expert assistant.

Users can record or upload bird vocalizations directly through the interface. These audio files are sent to the backend via a RESTful API, which returns prediction results indicating the most likely bird species to the LLM. The frontend then displays this information in a clear and accessible format, enabling users to further explore related content.

The React app is structured into several key sections:

- **Home Page**: Offers an overview of the appâ€™s capabilities and guides users to different features.

  <img src="images/Frontend1.png"  width="800">

- **Bird Sound Explorer**: A catalog of local bird species, complete with images and example vocalizations.

  <img src="images/Frontend4.png"  width="800">

- **Interactive Maps**: Displays geospatial data such as historical sighting locations, habitat conditions (e.g., forest cover, deforestation), and general biodiversity hotspots.

 <img src="images/Frontend5.png"  width="800">

- **Audio Upload & Chat Interface**: Allows users to submit recordings and engage with the LLM-based chatbot to ask questions and receive context-aware information about the identified bird species and other bird information of their interest.

<<<<<<< HEAD

## Running Dockerfile
=======
**3.1 Interactive Maps**

**3.2 App Containers**

The BirdWatching app requires the setup of three containers:

1. A vector database container 
2. A Python container for the API services
3. A web server container for the frontend

**Prerequisites**

A prerequisite is the installation of Docker. 

Each container will run in isolation but communicate with each other to create the complete BirdWatching application.

**Clone GitHub Repository**

Clone or download the github repository. 

**Create a Local Secrets Folder**

The secrets file is managed outside of GitHub since secure information should be maintained outside of GitHub. 

At the same level as the E115-BirdWatchingApp-Internal folder create a folder called secrets where you will add your service account. 

Your folder structure should look like this:
```
 |-E115-BirdWatchingApp-Internal
     |-images
     |-notebooks
     |-references
     |-reports
     |-src
       |---api-service
       |---frontend-react
       |---vector-db
  |-secrets
```
  
**Setup GCP Service Account**

1. To setup a service account you will need to go to GCP Console, search for "Service accounts" from the top search box or go to: "IAM & Admins" > "Service accounts" from the top-left menu and create a new service account called "ml-workflow". For "Service account permissions" select "Storage Admin", "AI Platform Admin", "Vertex AI Administrator", "Service Account User"
2. This will create a service account
3. On the right "Actions" column click the vertical ... and select "Manage keys". A prompt for Create private key for "ml-workflow" will appear select "JSON" and click create. This will download a Private key json file to your computer. Copy this json file into the secrets folder. Rename the json file to ml-workflow.json

**Vector DB Container**

We will set up and initialize our vector database with bird-related content for Retrieval Augmented Generation (RAG).

Set up the Vector Database:

1. Navigate to the vector-db directory:
   
cd E115â€”BirdWatchingAppâ€”Internal/src/vector-db

2. Build and run the container:
   
sh docker-shell.sh

3. Initialize the database. Run this within the docker shell:
   
python cli.py --download --load --chunk_type recursive-split

This process will:

- Download the bird knowledge base (chunks + embeddings)
- Load everything into the vector database 

This step is crucial for enabling the bird assistant to provide accurate, knowledge-based responses.

Keep this container running while setting up the backend API service and frontend apps.

**API-Service Container** 

We create a container running a FastAPI-based REST API service.

Setup API-Service:

1. Navigate to API Service Directory
   
cd E115â€”BirdWatchingAppâ€”Internal/src/api-service

2. Build & Run Container
   
sh docker-shell.sh

3. Review Container Configuration
   
-	Check docker-shell.sh:
-	Port mapping: -p 9000:9000
-	Development mode: -e DEV=1
-	Check docker-entrypoint.sh: Dev vs. Production settings

4. Start the API Service
   
- Run the following command within the docker shell:

uvicorn_server

- Verify the service is running at http://localhost:9000

- Enable API Routes

- Enable All Routes in api/service.py

**Additional routers here**

app.include_router(newsletter.router, prefix="/newsletters")
app.include_router(podcast.router, prefix="/podcasts")
app.include_router(llm_chat.router, prefix="/llm")
app.include_router(llm_cnn_chat.router, prefix="/llm-cnn")
app.include_router(llm_rag_chat.router, prefix="/llm-rag")
app.include_router(llm_agent_chat.router, prefix="/llm-agent")

â€¢	Go to http://localhost:9000/docs and test the newsletters routes
 
**View API Docs**

Fast API gives interactive API documentation and an exploration tool for free.

- Go to http://localhost:9000/docs
- You can test APIs from this tool
 
Keep this container running while setting up the backend API service and frontend apps.

**React Frontend Container**

Setup React Frontend Container:

1. Navigate to the React frontend directory:
   
cd E115â€”BirdWatchingAppâ€”Internal/src/frontend-react

2. Start the development container:
   
sh docker-shell.sh

3. First time only: Install the required Node packages

npm install

4. Launch Development Server

- Start the development server:
   
npm run dev

- View the app at: http://localhost:3000
   
Note: Make sure the API service container is running for full functionality

**Docker Cleanup**

Make sure you do not have any running containers and clear up an unused images. Complete the following steps: 

1. Run

docker container ls

2. Stop any container that is running

3. Run

docker system prune

4. Run

docker image ls


### 5. Notebooks/Reports ####
This folder contains code that is not part of container - for e.g: Application mockup, EDA, any ğŸ” ğŸ•µï¸â€â™€ï¸ ğŸ•µï¸â€â™‚ï¸ crucial insights, reports or visualizations.

**5.1 Web Scrapping and Data Versioning**

**5.2 Acoustid Model for Bird Identification**

The notebook demonstrated how the BirdNET model is used to predict bird species from bird audio input. the BirdNET model use its built-in preprocessor to chunk the input audio into fixed length pieces, convert each of small piece into a spectrogram, and pass to the neural network model to generate an embedding. So for an audio recording longer than fixed chunk length, we can get embedding and prediction for each chunk of data. In the Notebook, we ranked the prediction result according to the confidence level from high to lower. A threshold can be set to decide if the prediction confidence level is acceptable, if not, the embedding generated for the audio file can be passed to transfer learn model for evaluation if it is a rare speices were trained in transfer leanring. 

**5.3 Transfer Learning for Identification of Rare Bird Species**

**5.4. Interactive Maps**


### 6. Work in Progress ####
**6.1 Transfer learning**

**6.2 Other Topics**

The team identified the following future tasks for development and testing:

1. Update internal code baseline from using cheese to bird naming conventions
2. Problem resolution of an intermittent error for the chat input resulting in an Axios error
3. Implement CI for multiple containers once we complete the lectures on this topic


