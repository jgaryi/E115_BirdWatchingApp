### Internal notes for M5 (please do not delete this paragraph yet): A new birdnet_app folder has been added to the src directory. It contains a complete, self-contained setup for running BirdNET with transfer learning independently. Also, for running vector db: python cli.py --download --chunk --embed --load --chunk_type char-split ###
### Internal notes for M5, also add final presentation and Medium blog post to repo ###

## Project Milestone 5 - E115 - Birdwatching App
   
#### Project Milestone 5 Organization

```
â”œâ”€â”€ Readme.md
â”œâ”€â”€ images 
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ vector-db
â”‚   â”œâ”€â”€ api-service
â”‚   â”œâ”€â”€ birdnet_app
â”‚   â”œâ”€â”€ deployment
â”‚   â””â”€â”€ frontendreact
â”œâ”€â”€ .github 
â”‚   â””â”€â”€ workflows
â”‚       â””â”€â”€ data_scraping.yml   
â”‚       â””â”€â”€ data_sound_explorer.yml   
â”‚       â””â”€â”€ data_maps.yml   
â”‚       â””â”€â”€ unit_test.yml   
â”‚       â””â”€â”€ integration_test.yml  
â”‚       â””â”€â”€ lint.yml 
â”‚       â””â”€â”€ deploy.yml 
â”œâ”€â”€ tests   
â”‚   â””â”€â”€ test_unit.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ ansible   
â”œâ”€â”€ kubernetes   
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ BirdWatchingApp.ipynb
â”‚   â”œâ”€â”€ Acoustic_Monitoring_EDA.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Biodiversity.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Bird_Locations.ipynb
â”‚   â”œâ”€â”€ Interactive_Map_Deforestation.ipynb
â”‚   â”œâ”€â”€ TransferLearningModel.ipynb
â”‚   â”œâ”€â”€ SemanticScholar.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â””â”€â”€ preprocess_cv.py
â”œâ”€â”€ references
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ FinalPresentation.pdf
â”‚   â””â”€â”€ MediumPost(pdf or link)     
```

# E115 - Milestone 5 - Birdwatching App

**Team Members:** Jaqueline Garcia-Yi, Susan Urban, Yong Li, and Victoria Okereke

**Group Name:** Birdwatching App

**Project:**  
This project leverages AI to support bird species identification, using Yanachaga ChemillÃ©n National Park in Peru as a case study. The park is home to over 500 bird species, many of which are endemicâ€”found nowhere else in the world. The app is powered by BirdNET, an open-source AI-based acoustic model trained to identify bird species through their vocalizations. To enhance detection capabilities, particularly for rare or underrepresented species that are not yet recognized by BirdNET, we are integrating a custom transfer learning model trained specifically on rare species audio data. The app also features interactive maps enriched with environmental and habitat data, such as deforestation and biodiversity hotspots. Additionally, a large language model (LLM) agent serves as a virtual bird expert, offering detailed information about identified species and answering bird-related queries. This project highlights the integration of AI prediction models, natural language processing, and geospatial data for applications in ecotourism, environmental monitoring, and education.
<br><br>

----

## Milestone5 ##

In this milestone, we focused on Continuous Integration/Continuous Deployment (CI/CD) which is a set of principles and practices in software development and operations aimed at frequently delivering code changes reliably and efficiently.

From previous milestones, we have the frontend, API service, data management, as well as the interactive maps, and acoustic and language models.

### 1. Application Design ###

Before we start implementing the app we built a detailed design document outlining the applicationâ€™s architecture. We built a Solution Architecture and Technical Architecture to ensure all our components work together.

Here is our Solution Architecture:

<img src="images/solution-arch.png"  width="800">

Here is our Technical Architecture:

<img src="images/technical-arch.png"  width="800">

The architectures follow a state of the art design using enterprise COTS and open source products. A Google Earth API is added in support of the several maps used in the app. 


### 2. Backend API ###

The backend API is built using FastAPI and serves as the core interface between the frontend, the BirdNET model, and the LLM agent. It processes both audio and text inputs and routes them through intelligent workflows designed to enhance the birdwatching experience. It allows audio-based species detection by accepting bird audio recordings and processes them through the BirdNET model to identify the bird species. It also supports natural language inputs from users and sends them to the LLM agent for answers. It wraps BirdNET predictions with informative responses generated by the LLM agent, returning a rich description to the user. The backend API further provides endpoints for checking model status and overall API health.

<img src="images/api-list.png"  width="800">

**2.1. Acoustic Model for Bird Species Identification**

The acoustic model BirdNET is used for identification of bird species by audio recording. The frontend allows an audio file to be uploaded, and the backend api ```llm_cnn_chat``` saves the file in a temperory path and passes the temp file path to the BirdNET model for prediction. 
Key features: 
-   No preprocessing on audio file is required: The BirdNET model uses its built-in preprocessor to chunk the input audio into fixed length (3 or 5 seconds) pieces, and converts each of the small pieces into a spectrogram as an image input to a neural network in BirdNET.
-   Multiple audio formats supported: supports not only .mp3 and .wav, but also .flac
-   Long recording supported: for an audio recording longer than a fixed chunk length, the BirdNET model will generate embeddings and make a prediction on each chunk of data. The audio file size limition is 5MB.
-   Enhanced accuracy with longer data: The prediction on each data chunk is averaged on each species over all chunks. The results are ranked to obtain the prediction with the highest confidence. 
-   Augmented with text query: The api answers a question either when the audio is uploaded or after the model prediction.  The audio input about the species relates to habitat preferences, feeding behaviors and dietary needs, breeding cycles and nesting habits, conservation strategies and threats. 

**2.2. Bird Knowledge Expert (LLM-Agent Chatbot)**

The LLM agent is the conversational layer that brings context and insight to the app. Whether responding to text-based questions or enhancing audio-based bird species predictions, it serves up detailed, accurate, and engaging information.

Key features:
-   RAG-Enhanced Response Generation: The agent uses a Retrieval-Augmented Generation (RAG) architecture. It includes a custom tool that retrieves relevant bird species data from our web-scrapped, chunked, and embedded knowledge base, then injects the retrieved content into the LLM to generate informed, natural responses.
-   Audio Input Pipeline: When an audio file is received, the BirdNET model predicts the most likely species. This prediction is then used as a query to retrieve species information, which the agent uses to craft a natural, informative response.
-   Text Input Support: For direct user questions, the agent dynamically pulls and injects relevant information to respond meaningfully.
-   Tool-Augmented Architecture: Our agent uses the ```get_specie_info_by_search_content``` tool, which filters and ranks the knowledge base content using the cosine similarity against expanded user queries.


### 3. Frontend React ###
The frontend of the application is built as a user-friendly React web interface, designed to enable users to identify bird species through audio recordings and access detailed bird information via an AI agent. The primary functionality centers around integrating with a backend service that hosts a BirdNET-based acoustic model for species prediction and a large language model (LLM) acting as a bird expert assistant.

Users can record or upload bird vocalizations directly through the interface. These audio files are sent to the backend via a RESTful API, which returns prediction results indicating the most likely bird species to the LLM. The frontend then displays this information in a clear and accessible format, enabling users to further explore related content.

The React app is structured into several key sections:

- **Home Page**: Offers an overview of the appâ€™s capabilities and guides users to different features.

  <img src="images/Frontend1.png"  width="800">

- **Bird Sound Explorer**: A catalog of local bird species, complete with images and example vocalizations.

  <img src="images/Frontend4.png"  width="800">

- **Interactive Maps**: This section presents geospatial visualizations of ecological and environmental data, including historical bird sighting locations, habitat conditions (e.g., forest cover, deforestation), and biodiversity hotspots. 

  <img src="images/Frontend5.png"  width="800">

Users can interact with the maps by zooming in/out and toggling between multiple data layers, such as land cover classifications, deforestation patterns, and the boundaries of the protected area. These interactive features enable a deeper spatial understanding of bird habitats and environmental pressures within Yanachaga ChemillÃ©n National Park.

  <img src="images/Deforestation.png"  width="400">                                                                                                <img src="images/Deforestation2.png"  width="400">

- **Audio Upload & Chat Interface**: Allows users to submit recordings and engage with the LLM-based chatbot to ask questions and receive context-aware information about the identified bird species and other bird information of their interest.

  The interface allows uploading audios for bird identification:

  <img src="images/Uploading Audio.png"  width="800">

  And getting expert information about birds:

  <img src="images/Chatbot.jpg"  width="800">

### 4. Running CI/CD ###

**4.1 Continuous Integration (CI)**

The practice of regularly integrating code changes from multiple developers into a shared repository. The main goal is to detect integration issues early by automatically testing and building the code whenever a change is made. CI ensures that the codebase is always in a functional state.


**CI Prerequisites:**
- Developers satisfactorily complete all pylint(python), JSLint(JavaScript), Hadolint(Dockerfile), Black(formatter), and unit tests locally prior to commiting the code.

**CI Setup/Instructions:**

- Unit test (see report instructions_for_unit_integration_test.pdf)
- Integration test (see report instructions_for_unit_integration_test.pdf)
- GitHub action for integration test (check action on the python-test.yml)


**4.2 Prequistes and Setup for Deployment to GCP**

**Prerequisites:**

**API's to enable in GCP**

Search for each of these in the GCP search bar and click enable to enable these API's:
* Vertex AI API
* Compute Engine API
* Service Usage API
* Cloud Resource Manager API
* Google Container Registry API
* Kubernetes Engine API

**Setup GCP Service Account**

- To setup a service account, go to [GCP Console](https://console.cloud.google.com/home/dashboard), search for  "Service accounts" from the top search box or go to: "IAM & Admins" > "Service accounts" from the top-left menu and create a new service account called "deployment". 
- Give the following roles:
- For `deployment`:
    - Compute Admin
    - Compute OS Login
    - Container Registry Service Agent
    - Kubernetes Engine Admin
    - Service Account User
    - Storage Admin
    - Vertex AI Administrator
- Then click done.
- This will create a service account.
- On the right "Actions" column click the vertical ... and select "Create key". A prompt for Create private key for "deployment" will appear. Select "JSON" and click create. This will download a private key json file to your computer. Copy this json file into the **secrets** folder.
- Rename the json key file to `deployment.json`
- Follow the same process to create another service account called `gcp-service`
- For `gcp-service` give the following roles:
    - Storage Object Viewer
    - Vertex AI Administrator
- Then click done.
- This will create a service account.
- On the right "Actions" column click the vertical ... and select "Create key". A prompt for Create private key for "gcp-service" will appear select "JSON" and click create. This will download a Private key json file to your computer. Copy this json file into the **secrets** folder.
- Rename the json key file to `gcp-service.json`

**Setup Docker Container (Ansible, Docker, Kubernetes)**

Use Docker to build and run a standard container with all of the required software.

**Run `deployment` container**

- cd into `deployment`
- Go into `docker-shell.sh` and change `GCP_PROJECT` to your project id
- Run
```
sh docker-shell.sh
```

- Check to make sure you are authenticated to GCP
- Run
```
gcloud auth list
```

The Docker container connects to your GCP and can create VMs and deploy containers all from the command line.

**SSH Setup**

**Configuring OS Login for service account**
Run this command within the `deployment` container:
```
gcloud compute project-info add-metadata --project <YOUR GCP_PROJECT> --metadata enable-oslogin=TRUE
```

**Create SSH key for service account**
```
cd /secrets
ssh-keygen -f ssh-key-deployment
cd /app
```

**Providing public SSH keys to instances**
```
gcloud compute os-login ssh-keys add --key-file=/secrets/ssh-key-deployment.pub
```
From the output of the above command keep note of the username. Here is a snippet of the output 
```
- accountId: ac215-project
    gid: '3906553998'
    homeDirectory: /home/sa_100110341521630214262
    name: users/deployment@ac215-project.iam.gserviceaccount.com/projects/ac215-project
    operatingSystemType: LINUX
    primary: true
    uid: '3906553998'
	...
    username: sa_100110341521630214262
```
The username is `sa_100110341521630214262`

**Deployment Setup**
* Add ansible user details in inventory.yml file
* GCP project details in inventory.yml file
* GCP Compute instance details in inventory.yml file
* Replace project to your GCP project id in inventory_prod.yml
* Replace project to your GCP project id in docker-shell.sh

**4.3 Deployment**

**Build and Push Docker Containers to Google Artifact Registry**
```
ansible-playbook deploy-docker-images.yml -i inventory.yml
```

**Create Compute Instance (VM) Server in GCP**
```
ansible-playbook deploy-create-instance.yml -i inventory.yml --extra-vars cluster_state=present
```

Get the IP address of the compute instance from the GCP Console and update the appserver>hosts in the inventory.yml file

**Provision Compute Instance in GCP**
Install and setup for deployment.
```
ansible-playbook deploy-provision-instance.yml -i inventory.yml
```

**Setup Docker Containers in the  Compute Instance**
```
ansible-playbook deploy-setup-containers.yml -i inventory.yml
```

To get into a container, run:
```
sudo docker exec -it api-service /bin/bash
```

**Configure Nginx file for Web Server**
* Create the nginx.conf file for defaults routes in the web server

**Setup Webserver on the Compute Instance**
```
ansible-playbook deploy-setup-webserver.yml -i inventory.yml
```
Once the command runs go to `http://<External IP>/` 

**Delete the Compute Instance / Persistent disk**
```
ansible-playbook deploy-create-instance.yml -i inventory.yml --extra-vars cluster_state=absent
```

**Deployment with Scaling using Kubernetes**

In this section deploy the BirdWatching app to a K8s cluster

**API's to enable in GCP for Project**
If not previously completed, search for each of these in the GCP search bar and click enable to enable these API's:
* Vertex AI API
* Compute Engine API
* Service Usage API
* Cloud Resource Manager API
* Google Container Registry API
* Kubernetes Engine API

**Start Deployment Docker Container**
-  `cd deployment`
- Run `sh docker-shell.sh` or `docker-shell.bat` for windows
- Check versions of tools
`gcloud --version`
`kubectl version`
`kubectl version --client`

- Confirm authentication to GCP
- Run `gcloud auth list`

**Build and Push Docker Containers to GCR**
This step is only required if you have NOT already done this
```
ansible-playbook deploy-docker-images.yml -i inventory.yml
```

**Create & Deploy Cluster**
```
ansible-playbook deploy-k8s-cluster.yml -i inventory.yml --extra-vars cluster_state=present
```

This is how the various services communicate between each other in the Kubernetes cluster.

```mermaid
graph LR
    B[Browser] -->|nginx-ip.sslip.io/| I[Ingress Controller]
    I -->|/| F[Frontend Service<br/>NodePort:3000]
    I -->|/api/| A[API Service<br/>NodePort:9000]
    A -->|vector-db:8000| V[Vector-DB Service<br/>NodePort:8000]

    style I fill:#lightblue
    style F fill:#lightgreen
    style A fill:#lightgreen
    style V fill:#lightgreen
```

View the App
* Copy the `nginx_ingress_ip` from the terminal from the create cluster command
* Go to `http://<YOUR INGRESS IP>.sslip.io`

**Create Kubernetes Cluster**

**Create Cluster**
```
gcloud container clusters create test-cluster --num-nodes 2 --zone us-east1-c
```

**Deploy the App**
```
kubectl apply -f deploy-k8s-tic-tac-toe.yml
```

**Get the Loadbalancer external IP**
```
kubectl get services
```

**View the App**
* Copy the `External IP` from the `kubectl get services`
* Go to `http://<YOUR EXTERNAL IP>`


**4.4 Continuous Deployment (CD):** 

This takes the automation a step further by automatically deploying code changes to production after they pass all the automated tests in the deployment pipeline. This approach allows for a rapid release cycle and is commonly used in scenarios where rapid deployment and iteration are essential.

**Setup GitHub Action Workflow Credentials**

Setup credentials in GitHub to perform the following functions in GCP:
* Push docker images to GCR
* Run Vertex AI pipeline jobs
* Update kubernetes deployments 

**Setup**
* Go to the repo Settings
* Select "Secrets and variable" from the left side menu and select "Actions"
* Under "Repository secrets" click "New repository secret"
* Give the Name as "GOOGLE_APPLICATION_CREDENTIALS"
* For the Secret copy+paste the contents of your secrets file `deployment.json` 

**Frontend & Backend Changes**

See the [`reports/`](/reports/CICD_and_Kubernetes_Scaling.pdf) for our example implementation of a GitHub Action.

**View the App (If you have a domain)**
1. Get your ingress IP:
   * Copy the `nginx_ingress_ip` value that was displayed in the terminal after running the cluster creation command or from GCP console -> Kubernetes > Gateways, Services & Ingress > INGRESS

   * Example IP: `34.148.61.120`

2. Configure your domain DNS settings:
   * Go to your domain provider's website (e.g., GoDaddy, Namecheap, etc.)
   * Find the DNS settings or DNS management section
   * Add a new 'A Record' with:
     - Host/Name: `@` (or leave blank, depending on provider)
     - Points to/Value: Your `nginx_ingress_ip`
     - TTL: 3600 (or default)

3. Wait for DNS propagation (can take 5-30 minutes)

4. Access your app:
   * Go to: `http://your-domain.com`
   * Example: `http://formaggio.me`

**View the App (If you do not have a domain)**
* Copy the `nginx_ingress_ip` from the terminal from the create cluster command
* Go to `http://<YOUR INGRESS IP>.sslip.io`

* Example: http://35.231.159.32.sslip.io/

<hr style="height:4px;border-width:0;color:gray;background-color:gray">

**Delete Cluster**
```
gcloud container clusters delete test-cluster --zone us-east1-c
```

### 5. Usage details and examples. ###

[PLACEHOLDER]

### 6. Known issues and limitations. ###

Issues:

- Depending on the workstation, if it is a Mac or Windows or the version of the Mac (M1/M2/M4 chip), the instructions may need to be modified to run successfully. Since each workstation is configured differently or has different versions, the combination of changes are specific for that workstation. 

Limitations:

- the number of new birds (not already contained within the birdnet training set) is limited but will be growing with greater engagement with the Yanachaga ChemillÃ©n National Park staff in the future. The transfer learning capability is in place and can be easily extended with future additions. 

### 7. Notebooks/Reports ####
This folder contains code that is not part of a container e.g., Application mockup, EDA, any ðŸ” ðŸ•µï¸â€â™€ï¸ ðŸ•µï¸â€â™‚ï¸ crucial insights, reports or visualizations.

**7.1 Web Scrapping and Data Versioning**

We collected bird species information by scraping text from three authoritative websites, along with scholarly articles sourced from Google Scholar and Semantic Scholar. The extracted data was cleaned and structured before being stored in a GCS bucket for use in downstream processing and semantic search.

An initial baseline of both the acoustic data and the LLM-RAG data was collected in March 2025. The data was scraped from the sources mentioned above using custom scripts.

Over the course of several months, additional data may be incorporated into both the acoustic and LLM datasets. However, for the remote sensing model, relevant information such as deforestation changes is typically updated once per year.

Both the acoustic and LLM-RAG datasets are dynamic, with snapshots taken at specific intervals to capture full replacements of the data. As website authors may update or expand the data, these datasets are expected to evolve over time, with content updates occurring over several months.

Previous versions of the models are not expected to be revisited. Given that updates are anticipated to occur every few months, the team will establish the initial baseline and later implement the Data Versioning Container.

**Files:**
- **cli.py**: This script scrapes text and images and stores the data in GCS bucket. It handles the following:
   -   Text Data -> Saved as .txt files in the bird_description folder.
   -   Image Data -> Stored in the bird_images folder.
   -   Audio Data -> Manually added to the acoustic_data folder.
- **preprocess_cv.py**: This script processes the images collected in bird_images by resizing them to 128x128 pixels and uploading the resized images to the resized folder in GCS bucket.
- **semanticscholar.py**: This script scrapes PDF articles from the semantic scholar website.

**7.2 Acoustic Model for Bird Identification**

The notebook demonstrated how the BirdNET model is used to predict bird species from bird audio input. The BirdNET model use its built-in preprocessor to chunk the input audio into fixed length pieces, convert each of small piece into a spectrogram by Short Time Fourier Transformation. The spectrogram is represented in both time and frequency domain, can be treated as "image" data (see Notebook). It passes it to the neural network model BirdNET to generate an embedding. 

So for an audio recording longer than the fixed chunk length, we obtain embeddings and a prediction for each chunk of data. In the notebook, we ranked the prediction result according to the confidence level from high to low. A threshold can be set to decide if the prediction confidence level is acceptable, if not, the embedding generated for the audio file can be passed to the transfer model for evaluation if it is a rare species. 

**Files:**
- **Acoustic_Monitoring_EDA.ipynb**: This notebook discussed the bird song acoustic representation in time, and time-frequency domain as an "image". 

- **BirdWatchingApp.ipynb**: This notebook illustrates how to use the BirdNET model for bird species prediction, and ranking of the result according to the prediction confidence. 

**7.3 Transfer Learning for Identification of Rare Bird Species**   

In this project, we develop a transfer learning approach to classify rare bird species based on audio recordings. The transfer learning workflow begins by preprocessing field recordings, followed by extracting acoustic embeddings using the BirdNET model. These embeddings serve as feature vectors for training lightweight classifiers capable of recognizing underrepresented species such as Doliornis and Hapalopsittaca.

We implement and compare several modelsâ€”including multinomial logistic regression, K-nearest neighbors (KNN), multilayer perceptrons (MLPs), and a few-shot learning strategy based on cosine similarity. Cross-validation using grouped audio samples ensures robust evaluation without data leakage. The MLP model achieved the best performance, with an average cross-validation accuracy of 0.87. Based on this result, the MLP was retrained on the full dataset for both rare species, and the notebook includes a practical example demonstrating how to use the final model for prediction.

**Files:**
- **TransferLearningModel.ipynb**: This is the main notebook containing the full transfer learning pipeline. Note: GitHub may display it as an "invalid notebook" due to a known glitch that occasionally occurs when exporting notebooks from Google Colab.

- **TransferLearningModel.pdf**: A static PDF version of the notebook is provided for easier viewing directly on GitHub.

**7.4. Interactive Maps**  

Three interactive maps provide complementary geospatial insights into the biodiversity and habitat conditions of Yanachaga-ChemillÃ©n National Park, using Earth Engine and Folium (a python interface to Leaflet) for visualization. The **bird location map** displays historical sighting data for selected endemic and rare species, enabling users to explore species distribution patterns within the protected area. The **deforestation map** overlays land cover classifications (ESA WorldCover) and forest loss data (Hansen Global Forest Change), helping users assess environmental pressures on bird habitats. The **biodiversity hotspot map** highlights the Tropical Andes and other global conservation priority regions, showing the parkâ€™s position within a high-priority biodiversity corridor. Users can interact with the maps by zooming in/out and toggling between multiple data layers, such as land cover, deforestation, species presence, and park boundaries, supported by custom legends for easy interpretation.

**Files:**
- **InteractiveMapBird_Locations.ipynb**: Displays historical sighting locations of selected bird species within Yanachaga-ChemillÃ©n National Park

- **InteractiveMapDeforestation.ipynb**: Visualizes land cover and forest loss data over the protected area

- **InteractiveMapBiodiversity.ipynb**: Highlights global biodiversity hotspots with a focus on the Tropical Andes, showing the parkâ€™s placement within a major conservation priority zone

**7.5. Linters**

**7.6. Unit Test**
The unit test will test each individual functions in birdnet_app container in isolation with mocked dependencies. In the unit test, we use simulated audio input instead of real one. Below are all test cases in unit test, and the instruction to run unit test is shown in the report listed below. 
  1.	Audio Preprocessing: (a) Test padding of short audio. (b) Test of long audio over limit (c) Test handling of invalid audio
  2.	BirdNET Analysis:  (a) Test successful detection  (b) Test no detection case.  (c) Test error handling
  3.	Custom Model Classification:  (a) Test known species detection. (b) Test unknown species case
  4.	Full Pipeline:  (a) Test BirdNET primary path. (b)Test fallback to custom model

**Files:**
- **test_unit.py**: covers all unit test cases list above to verify the functionality of all functions in the birdnet_app container. 

- **instructions_for_unit_integration_test.pdf**: Explain the unit test in details and the instructions to run the test_unit.py by pytest.

**7.7. Integration Test**
The integration test will test the interaction between functions. In the integration test, both simulated and real audio input can be used during the test, but real audio inputs are commonly used. Below are all test cases in the integration test, and the instruction to run the test is shown in the report listed below.
1.	Test the full pipeline with real audio: Verify the entire workflow from audio upload to species prediction
2.	API integration test with real request: Test the FastAPI endpoint with real HTTP call and actual audio processing
3.	Test edge cases in integration: Verify real-world scenarios that unit test might miss
4.	Cross-validate BirdNET and Custom model: Ensure both models agree on simple case as it expected
5.	Performance testing: Check if the system meets real-time requirement

**Files:**
- **test_integration.py**: covers all integration test cases (with real audios) listed above to verify the interaction between functions in birdnet_app. 

- **test_integration_mock.py**: covers all integration test cases (with simulated audios) listed above to verify the interaction between functions in birdnet_app. 

- **instructions_for_unit_integration_test.pdf**: Explain the unit test in details and the instructions to run the test_integration.py by pytest.

### 8. Future Works ####

**8.1 Improve App Functionalities**

The birdwatching app develped implemented basic function for bird speices detection and chat with agent about the bird species detected. More functionalities can be considered for futher steps.

1. Extend the number of training species for transfer learning: current transfer learning model is trained with two real species. more data from other speices could be available to retrain the model and extend the model capabilty. 
2. 


**8.2 Adaptation for Deployment to Edge or IoT Device**

One of primary further goal of this project is to deploy the birdwatching app to edge devices located in remote national park for monitoring the bird species continuously as ecosystem perservation purpose. The edge or embedded device have limited source: power, computation and memory. SO we need to do some adaptations to enable this deployment. 

1. software adaptation includes: lightweight container such as using minimal base image, only ship runtime dependencies.  
2. Replace GKE with k3s (lightweight kubernetes) for edge orchestration.
3. Apply techniques to achieve low-power computation, such as select edge computer for efficient Tensorflow Lite model execution and use model with low-bit quantization to reduce compute load. 






