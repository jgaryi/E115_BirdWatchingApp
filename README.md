## Project Milestone 4 - E115 - Birdwatching App
   
#### Project Milestone 4 Organization

```
├── Readme.md
├── images 
├── notebooks
│   ├── BirdWatchingApp.ipynb
│   ├── Acoustic_Monitoring_EDA.ipynb
│   ├── Interactive_Map_Biodiversity.ipynb
│   ├── Interactive_Map_Bird_Locations.ipynb
│   ├── Interactive_Map_Deforestation.ipynb
│   ├── SemanticScholar.py
│   ├── cli.py
│   └── preprocess_cv.py
│   
├── references
├── reports
│   ├── BirdWatchingAppMidterm.pdf
│   └── Statement of Work_Sample.pdf 
│
└── src  
    ├── api-service
    │    ├── api
    │    │   ├── routers
    │    │   ├── utils
    │    │   ├── service.py
    │    ├── docker-entrypoint.sh
    │    ├── docker-shell.sh
    │    ├── Dockerfile
    │    ├── Pipfile
    │    ├── Pipfile.lock
    ├── frontend-react
    │    ├── public
    │    ├── src
    │    ├── Dockerfile
    │    ├── docker-shell.sh
    └── vector-db        

```

# E115 - Milestone4 - Birdwatching App

**Team Members:** Jaqueline Garcia-Yi, Susan Urban, Yong Li, and Victoria Okereke

**Group Name:** Birdwatching App

**Project:**  
This project aims to develop an AI-powered bird identification and knowledge application, using the Yanachaga Chemillen National Park in Peru as a case study. The app will offer two primary methods for bird recognition: (1) users can upload a sound file for automatic identification, powered by an acoustic AI model, or (2) manually search for a bird by its scientific name using a dropdown menu to select from one of nine species.<br>

If the identified bird is one of the 500 species found in the national park but not listed in the dropdown menu, the app will display only the species name and return to the initial page when clicked again. If the bird is one of the nine pre-selected species—whose populations are vulnerable and decreasing—the app will provide detailed information about the species, its migratory path, and an image for visual reference. <br>

Interactive maps will show the bird's previously identified locations within the national park, habitat details, and changes over time that help explain why these species are vulnerable or endangered. Another map will also highlight other areas where the bird is likely to be found, based on predictions from a geo-referenced model using remote sensing data. Additionally, a built-in chatbot will allow users to ask bird-related questions and receive answers powered by a Large Language Model with Retrieval-Augmented Generation (LLM-RAG).<br>

The statement of work and lastest version of the project wireframe are available here: [`reports/`](reports/)  
<br><br>

----

## Milestone4 ##

In this milestone, we have the components for frontend, API service, also components from previous milestones for data management, including versioning, as well as the interactive maps, and acoustic and language models.

After completions of building a robust ML Pipeline in our previous milestone we have built a backend api service and frontend app. This will be our user-facing application that ties together the various components built in previous milestones.

### 1. Application Design ###

Before we start implementing the app we built a detailed design document outlining the application’s architecture. We built a Solution Architecture and Technical Architecture to ensure all our components work together.

Here is our Solution Architecture:

<img src="images/solution-arch.png"  width="800">

Here is our Technical Architecture:

<img src="images/technical-arch.png"  width="800">

The architectures follow a state of the art design using enterprise COTS and open source products. A Google Earth API is added in support of the several maps used in the app. 

<<<<<<< HEAD
<<<<<<< HEAD

**Backend API**
=======
=======

>>>>>>> bd1f5cc3af336ff388b60770c88bb8e530c3ad53
### 2. Backend API ###
>>>>>>> a60db3e4c99205c015520f1ad3e40bd4c2b3ac47

The backend API is built using FastAPI and serves as the core interface between the frontend, the BirdNET model, and the LLM agent. It processes both audio and text inputs and routes them through intelligent workflows designed to enhance the birdwatching experience. 
Key responsibilities include:
-   Audio-Based Species Detection: It accepts bird audio recordings and processes them through the BirdNET model to identify the bird species.
-   Text Query Handling: It supports natural language inputs from users and sends them to the LLM agent for answers.
-   Unified Response API: It wraps BirdNET predictions with informative responses generated by the LLM agent, returning a rich description to the user.
-   Metadata & Diagnostics: It provides endpoints for checking model status and overall API health.

<img src="images/api-list.png"  width="800">

**2.1. Acoustic Model for Bird Species Identification**

The frontend will allow an audio file uploaded with a limitation of 5MB, and the backend api llm_cnn_chat.py will save the file in a temperory path and pass the temp file path to BirdNET model. Without any preprocessing, the BirdNET model use its built-in preprocessor to chunk the input audio into fixed length (3 or 5 seconds) pieces, and convert each of small piece into a spectrogram by Short Time Fourier Transformation. The spectrogram is represented in both time and frequency domain, can be treated as "image" data (see 5.2 Notebook). Each image data chunk pass to the BirdNET neural network model and generate an embedding. So for an audio recording longer than fixed chunk length, the BirdNET model will generate more than one embeddings, and give prediction for each embedding or each chunk of data. The prediction confidence on each species are averaged over all chunks, and the results are ranked and get the prediction with highest confidence. The api llm_cnn_chat could answer question about the species related to habitat preferences, feeding behaviors and dietary needs, breeding cycles and nesting habits, conservation strategies and threats etc. The questions can be either when the audio is uploaded or after the model predict the audio input. 

**2.2. Bird Knowledge Expert (LLM-Agent Chatbot)**

The LLM agent is the conversational layer that brings context and insight to the app. Whether responding to text-based questions or enhancing audio-based bird species predictions, it serves up detailed, accurate, and engaging information.
Core features include:

### 3. Frontend React ###

A user friendly React app was built to identify various species of mushrooms in the wild using computer vision models from the backend. Using the app a user can take a picture of a mushroom and upload it. The app will send the image to the backend api to get prediction results on weather the mushroom is poisonous or not. 

Here are some screenshots of our app:

```Add screenshots here```

<<<<<<< HEAD

## Running Dockerfile
=======
**3.1 Interactive Maps**

**3.2 **

### 4. Running Dockerfile ###




### 5. Notebooks/Reports ####
This folder contains code that is not part of container - for e.g: Application mockup, EDA, any 🔍 🕵️‍♀️ 🕵️‍♂️ crucial insights, reports or visualizations.

**5.1 Web Scrapping and Data Versioning**

**5.2 Acoustid Model for Bird Identification**

The notebook demonstrated how the BirdNET model is used to predict bird species from bird audio input. the BirdNET model use its built-in preprocessor to chunk the input audio into fixed length pieces, convert each of small piece into a spectrogram, and pass to the neural network model to generate an embedding. So for an audio recording longer than fixed chunk length, we can get embedding and prediction for each chunk of data. In the Notebook, we ranked the prediction result according to the confidence level from high to lower. A threshold can be set to decide if the prediction confidence level is acceptable, if not, the embedding generated for the audio file can be passed to transfer learn model for evaluation if it is a rare speices were trained in transfer leanring. 

**5.3 Transfer Learning for Identification of Rare Bird Species**

**5.4. Interactive Maps**


### 6. Work in Progress ####
**6.1 Transfer learning**

**6.2 Other Topics**

The team identified the following future tasks for development and testing:

1. Update internal code baseline from using cheese to bird naming conventions
2. Problem resolution of an intermittent error for the chat input resulting in an Axios error
3. Implement CI for multiple containers once we complete the lectures on this topic


